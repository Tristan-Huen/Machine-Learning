{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2f2220c-32b6-4122-b568-282680e31fd2",
   "metadata": {},
   "source": [
    "# Kernel Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ec09b57-f77a-4ac8-869a-83075ce358a3",
   "metadata": {},
   "source": [
    "## Feature Maps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ae7763-6e85-44e8-a529-03e7637cef29",
   "metadata": {},
   "source": [
    "Consider the case of fitting non-linear functions of $x$ such as $y=\\theta_3 x^3 + \\theta_2 x^2 + \\theta_1 x + \\theta_0$. We can view the cubic function as a linear function over a different set of feature variables. Let $\\phi:\\mathbb{R}\\to\\mathbb{R}^4$ be defined as\n",
    "\n",
    "$$\\boldsymbol{\\phi}(x) = \\begin{bmatrix} 1 \\\\ x \\\\ x^2 \\\\ x^3 \\end{bmatrix} \\in \\mathbb{R}^4$$\n",
    "\n",
    "Let $\\boldsymbol{\\theta} \\in \\mathbb{R}^4$ be the vector containing $\\theta_0,\\theta_1,\\theta_2,\\theta_3$ as entries. Then, we can rewrite the cubic function as\n",
    "\n",
    "$$\\theta_3 x^3 + \\theta_2 x^2 + \\theta_1 x + \\theta_0 = \\boldsymbol{\\theta}^{T}\\boldsymbol{\\phi}(x)$$\n",
    "\n",
    "Thus, a cubic function of the variable $x$ can be viewed as linear function over the variables $\\boldsymbol{\\phi}(x)$. We will distinguish between these two sets of variables in the context of kernel methods by calling the \"original\" input value the input attributes of a problem ($x$). When the original input is mapped to some new set of quantities $\\boldsymbol{\\phi}(x)$, the new quantities will be called the features variables. The $\\boldsymbol{\\phi}$ is called a feature map with maps attributes to the features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c182bcda-5832-4d78-842c-91b85fdcd29d",
   "metadata": {},
   "source": [
    "## Least Mean Squares with Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1acb43f5-64de-4278-99c8-628800e7164a",
   "metadata": {},
   "source": [
    "Recall that the batch graident descent update rule for the least mean squares (LMS) was (note we leave the 1/m for simplicity)\n",
    "\n",
    "$$\\boldsymbol{\\theta}:= \\boldsymbol{\\theta} - \\alpha\\sum_{i=1}^{m}\\left(h_{\\theta}(\\boldsymbol{x}^{(i)})-y^{(i)}\\right)\\boldsymbol{x}^{(i)}$$\n",
    "\n",
    "where $h_\\theta (\\boldsymbol{x}) = \\boldsymbol{\\theta}^{T}\\boldsymbol{x}$. We wil derive the gradient descent algorithm for fitting the model $\\boldsymbol{\\theta}^{T}\\boldsymbol{\\phi}(x)$\n",
    "\n",
    "Let $\\phi:\\mathbb{R}^{n}\\to\\mathbb{R}^{p}$ which maps attribute $\\boldsymbol{x} \\in \\mathbb{R}^{n}$ to the features $\\boldsymbol{\\phi}(\\boldsymbol{x}) \\in \\mathbb{R}^{p}$. We can then replace all occurences of $\\boldsymbol{x}^{(i)}$ with $\\boldsymbol{\\phi}(\\boldsymbol{x}^{(i)})$ to get \n",
    "\n",
    "$$\\boldsymbol{\\theta}:= \\boldsymbol{\\theta} - \\alpha\\sum_{i=1}^{m}\\left(\\boldsymbol{\\theta}^{T}\\boldsymbol{\\phi}(\\boldsymbol{x}^{(i)})-y^{(i)}\\right)\\boldsymbol{\\phi}(\\boldsymbol{x}^{(i)})$$\n",
    "\n",
    "Similarly, the corresponding stochastic gradient descent (SGD) update rule is\n",
    "\n",
    "$$\\boldsymbol{\\theta}:= \\boldsymbol{\\theta} - \\alpha\\left(\\boldsymbol{\\theta}^{T}\\boldsymbol{\\phi}(\\boldsymbol{x}^{(i)})-y^{(i)}\\right)\\boldsymbol{\\phi}(\\boldsymbol{x}^{(i)})$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4809fc70-398c-4b58-8dcf-c534cb095227",
   "metadata": {},
   "source": [
    "## LMS with the Kernel Trick"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44e4b789-b9d0-4791-80ff-2759a71bdbed",
   "metadata": {},
   "source": [
    "The gradient descent or SGD update above becomes computationally expensive when the features is high-dimensional. To deal with this we will introduce the kernel trick.\n",
    "\n",
    "For simplicity, assume that we initialize $\\boldsymbol{\\theta}=0$ and focus on the update rule from before. We claim that at any time, $\\boldsymbol{\\theta}$ can be represented as a linear combinatio  of the vectors $\\boldsymbol{\\phi}(\\boldsymbol{x}^{(1)}),\\ldots,\\boldsymbol{\\phi}(\\boldsymbol{x}^{(m)})$. We can prove this inductively as follows. At initialization, $\\boldsymbol{\\theta}=0 = \\sum_{i=1}^{m}0\\cdot\\boldsymbol{\\phi}(\\boldsymbol{x}^{(i)})$. Assume at some point $\\boldsymbol{\\theta}$ can be represented as\n",
    "\n",
    "$$\\boldsymbol{\\theta} = \\sum_{i=1}^{m}\\beta_i \\boldsymbol{\\phi}(\\boldsymbol{x}^{(i)})$$\n",
    "\n",
    "for some $\\beta_1,\\ldots,\\beta_m \\in \\mathbb{R}$. In the next update, $\\boldsymbol{\\theta}$ is still a linear combination of $\\boldsymbol{\\phi}(\\boldsymbol{x}^{(1)}),\\ldots,\\boldsymbol{\\phi}(\\boldsymbol{x}^{(m)})$ since,\n",
    "\n",
    "\\begin{align}\n",
    "\\boldsymbol{\\theta}:&= \\boldsymbol{\\theta} - \\alpha\\sum_{i=1}^{m}\\left(\\boldsymbol{\\theta}^{T}\\boldsymbol{\\phi}(\\boldsymbol{x}^{(i)})-y^{(i)}\\right)\\boldsymbol{\\phi}(\\boldsymbol{x}^{(i)}) \\\\[3pt]\n",
    "&= \\sum_{i=1}^{m}\\beta_i \\boldsymbol{\\phi}(\\boldsymbol{x}^{(i)}) - \\alpha\\sum_{i=1}^{m}\\left(\\boldsymbol{\\theta}^{T}\\boldsymbol{\\phi}(\\boldsymbol{x}^{(i)})-y^{(i)}\\right)\\boldsymbol{\\phi}(\\boldsymbol{x}^{(i)}) \\\\[3pt]\n",
    "&= \\sum_{i=1}^{m}\\left(\\beta_i-\\alpha\\left(\\boldsymbol{\\theta}^{T}\\boldsymbol{\\phi}(\\boldsymbol{x}^{(i)})-y^{(i)}\\right)\\right)\\boldsymbol{\\phi}(\\boldsymbol{x}^{(i)})\n",
    "\\end{align}\n",
    "\n",
    "This new quantity in brackets is our new $\\beta_i$. The general strategy is then to represent the $p$-dimensional vector $\\boldsymbol{\\theta}$ by a set of coefficients $\\beta_1,\\ldots,\\beta_m$ and from here we can derive the update rule for these coefficients. From the above the update rule is\n",
    "\n",
    "$$\\beta_i := \\beta_i-\\alpha\\left(\\boldsymbol{\\theta}^{T}\\boldsymbol{\\phi}(\\boldsymbol{x}^{(i)})-y^{(i)}\\right) $$\n",
    "\n",
    "Here the old $\\boldsymbol{\\theta}$ is still on the RHS and we can replace it by $\\boldsymbol{\\theta} = \\sum_{j=1}^{m}\\beta_j \\boldsymbol{\\phi}(\\boldsymbol{x}^{(j)})$ which gives\n",
    "\n",
    "$$\\forall i \\in \\{1,\\ldots,m\\},\\beta_i := \\beta_i-\\alpha\\left(\\sum_{j=1}^{m}\\beta_j \\boldsymbol{\\phi}(\\boldsymbol{x}^{(j)})^{T}\\boldsymbol{\\phi}(\\boldsymbol{x}^{(i)})-y^{(i)}\\right)$$\n",
    "\n",
    "We often rewrite $\\boldsymbol{\\phi}(\\boldsymbol{x}^{(j)}) ^{T}\\boldsymbol{\\phi}(\\boldsymbol{x}^{(i)})$ as $\\langle\\boldsymbol{\\phi}(\\boldsymbol{x}^{(j)}),\\boldsymbol{\\phi}(\\boldsymbol{x}^{(i)})\\rangle$ to emphasize that it's the inner product of the two feature vectors. Viewing $\\beta_i\\,$'s as the new representation of $\\boldsymbol{\\theta}$, we now have turned the batch gradient descent into an algorithm that updates $\\beta$ iteratively. Though it may appear that at every iteration the values of the inner product must be computed for all pairs of $i,j$, each of which may take roughly $\\mathcal{O}(p)$ operation, two important properties help this out:\n",
    "\n",
    "1. We can pre-compute the pairwise inner products $\\langle\\boldsymbol{\\phi}(\\boldsymbol{x}^{(j)}),\\boldsymbol{\\phi}(\\boldsymbol{x}^{(i)})\\rangle$ for all $i,j$ before the loop starts.\n",
    "\n",
    "2. For many feature maps computing $\\langle\\boldsymbol{\\phi}(\\boldsymbol{x}^{(j)}),\\boldsymbol{\\phi}(\\boldsymbol{x}^{(i)})\\rangle$ can be efficient and does not necessarily require computing $\\boldsymbol{\\phi}(\\boldsymbol{x}^{(i)})$ explicitly. Consider the example where $\\boldsymbol{x} \\in \\mathbb{R}^{n}$ and let $\\boldsymbol{\\phi}(\\boldsymbol{x})$ contains all monomials of $\\boldsymbol{x}$  with degree $\\leq 3$. The dimension of the features is on the order of $n^3$\n",
    "\n",
    "\\begin{align}\n",
    "\\langle \\boldsymbol{\\phi}(\\boldsymbol{x}),\\boldsymbol{\\phi}(\\boldsymbol{z})\\rangle &= 1 + \\sum_{i=1}^{n} x_i z_i + \\sum_{i,j\\in\\{1,\\ldots,n\\}} x_i x_j z_i z_j + \\sum_{i,j,k\\in\\{1,\\ldots,n\\}} x_i x_j x_k z_i z_j z_l \\\\[3pt]\n",
    "&= 1 + \\sum_{i=1}^{n} x_i z_i + \\left(\\sum_{i=1}^{n} x_i z_i\\right)^2 + \\left(\\sum_{i=1}^{n} x_i z_i\\right)^3 \\\\[3pt]\n",
    "&= 1 + \\langle x,z \\rangle + \\langle x,z \\rangle^2 + \\langle x,z \\rangle^3\n",
    "\\end{align}\n",
    "\n",
    "So we can compute $\\langle x,z \\rangle$ with $\\mathcal{O}(n) time$ and then take another constant number of operations to compute the last line above.\n",
    "\n",
    "We define the Kernel corresponding to the feature map $\\boldsymbol{\\phi}$ as a function that maps $\\mathcal{X} \\times \\mathcal{X} \\to \\mathbb{R}$ satisfying\n",
    "\n",
    "$$K(\\boldsymbol{x},\\boldsymbol{z}) \\overset{\\Delta}{=} \\langle \\boldsymbol{\\phi}(\\boldsymbol{x}),\\boldsymbol{\\phi}(\\boldsymbol{z})\\rangle$$\n",
    "\n",
    "The final algorithm is as follows\n",
    "\n",
    "1. Compute all the values $$K(\\boldsymbol{x}^{(i)},\\boldsymbol{x}^{(j)}) \\overset{\\Delta}{=} \\langle\\boldsymbol{\\phi}(\\boldsymbol{x}^{(i)}),\\boldsymbol{\\phi}(\\boldsymbol{x}^{(j)})\\rangle$$ for all $i,j \\in {1,\\ldots,m}$. Set $\\beta:=0$\n",
    "\n",
    "2. Loop:\n",
    "\n",
    "$$\\forall i \\in \\{1,\\ldots,m\\},\\beta_i := \\beta_i-\\alpha\\left(\\sum_{j=1}^{m}\\beta_j K(\\boldsymbol{x}^{(i)},\\boldsymbol{x}^{(j)})-y^{(i)}\\right)$$\n",
    "\n",
    "or, if we let $K$ be the $m\\times m$ matrix with $K_ij = K(\\boldsymbol{x}^{(i)},\\boldsymbol{x}^{(j)})$, we have\n",
    "\n",
    "$$\\boldsymbol{\\beta} := \\boldsymbol{\\beta} + \\alpha(\\vec{y}-\\boldsymbol{K}\\boldsymbol{\\beta})$$\n",
    "\n",
    "Additonally, we can show that\n",
    "\n",
    "$$\\boldsymbol{\\theta}^{T}\\boldsymbol{\\phi}(\\boldsymbol{x}) = \\sum_{i=1}^{m}\\beta_i \\boldsymbol{\\phi}(\\boldsymbol{x}^{(i)})^{T}\\boldsymbol{\\phi}(\\boldsymbol{x}) = \\sum_{i=1}^{m} \\beta_i K(\\boldsymbol{x}^{(i)},\\boldsymbol{x})$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae87171e-6f75-48bc-87c3-8a8f9bb29327",
   "metadata": {},
   "source": [
    "## Properties of Kernels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba54d21-a713-40b7-b345-c84f19fcdd68",
   "metadata": {},
   "source": [
    "We saw that the kernel function when defined allowed the whole training algorithm to be written entirely in the language of the kernel without referring to the feature map and also allowed the hypothesis to be written in terms of it.\n",
    "\n",
    "It is then natural to try and define other kernel functions and run the algorithm. What kinds of functions $K(\\cdot,\\cdot)$ can correspond to some feature map $\\boldsymbol{\\phi}$?\n",
    "\n",
    "Answering this question allows us to complete change the interface of selecting feature maps to the interface of selecting kernel function $K$. A benefit is that we don't have to be able to compute $\\boldsymbol{\\phi}$ or write it down analytically, and we only need to know its existence.\n",
    "\n",
    "Suppose $\\boldsymbol{x},\\boldsymbol{z} \\in \\mathbb{R}^{n}$ and first consider the function$K(\\cdot,\\cdot)$ defined as \n",
    "\n",
    "$$K(\\boldsymbol{x},\\boldsymbol{z}) = (\\boldsymbol{x}^{T}\\boldsymbol{z})^2$$\n",
    "\n",
    "We can also write this as\n",
    "\n",
    "\\begin{align}\n",
    "K(\\boldsymbol{x},\\boldsymbol{z}) &= \\left(\\sum_{i=1}^{n}x_i z_i\\right)\\left(\\sum_{j=1}^{n}x_j z_j\\right) \\\\[3pt]\n",
    "&= \\sum_{i=1}^{n} \\sum_{j=1}^{n} x_i x_j z_i z_j \\\\[3pt]\n",
    "&=  \\sum_{i,j=1}^{n} (x_i x_j)(z_i z_j)\n",
    "\\end{align}\n",
    "\n",
    "Thus, we that $K(\\boldsymbol{x},\\boldsymbol{z}) = \\langle \\boldsymbol{\\phi}(\\boldsymbol{x}),\\boldsymbol{\\phi}(\\boldsymbol{z})\\rangle$ is the kernel function that corresponds to the feature mapping $\\boldsymbol{\\phi}$ whose elements are given by every pairwise permutation of $x_i x_j$. We can note that calculating the high-dimensional $\\boldsymbol{\\phi}(\\boldsymbol{x})$ requires $\\mathcal{O}(n^2)$ time, while finding $K(\\boldsymbol{x},\\boldsymbol{z})$ takes only $\\mathcal{O}(n)$ time.\n",
    "\n",
    "As another example consider $K(\\cdot,\\cdot)$ defined by\n",
    "\n",
    "\\begin{align}\n",
    "K(\\boldsymbol{x},\\boldsymbol{z}) &= (\\boldsymbol{x}^{T}\\boldsymbol{z}+c)^2 \\\\[3pt]\n",
    "&= \\sum_{i,j=1}^{n} (x_i x_j)(z_i z_j) + \\sum_{i=1}^{n} (\\sqrt{2c}x_i)(\\sqrt{2c}z_i) + c^2\n",
    "\\end{align}\n",
    "\n",
    "Again this gives feature mapping whose elements are given by every pairwise permutation of $x_i x_j$, $\\sqrt{2c}x_i$, and a single element $c$ which controls the relative weighting between the $x_i$ (first order) and $x_i x_j$ (second order) terms.\n",
    "\n",
    "The kernel $K(\\boldsymbol{x},\\boldsymbol{z}) = (\\boldsymbol{x}^{T}\\boldsymbol{z}+c)^k$ corresponds to a feature mapping to an $\\binom{n+k}{k}$ feature space, corresponding of all monomials of the form $x_{i_1}x_{i_2}\\ldots x_{i_k}$ that are up to order $k$. However, computing the kernel still takes only $\\mathcal{O}(n)$ time compared to the feature map taking $\\mathcal{O}(n^k)$ time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c0ea30c-7384-46c4-a7ae-da11e0692219",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Kernels as Similarity Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d9a280-0016-4770-830f-e3c7c603b3e5",
   "metadata": {},
   "source": [
    "If $\\boldsymbol{\\phi}(\\boldsymbol{x})$ and $\\boldsymbol{\\phi}(\\boldsymbol{x})$ are close together then one might expect the kernel function to be large. Conversely, if $\\boldsymbol{\\phi}(\\boldsymbol{x})$ and $\\boldsymbol{\\phi}(\\boldsymbol{x})$ are far apart then the kernel function would be small. So, the kernel can be thought of as some measurement of how similar $\\boldsymbol{\\phi}(\\boldsymbol{x})$ and $\\boldsymbol{\\phi}(\\boldsymbol{x})$ are, or how similar $\\boldsymbol{x}$ and $\\boldsymbol{z}$ are.\n",
    "\n",
    "With this intuition, one could try and build some kernel that would reasonably measure how similar $\\boldsymbol{x}$ and $\\boldsymbol{z}$ are. For example,\n",
    "\n",
    "$$K(\\boldsymbol{x},\\boldsymbol{z}) = \\exp\\left(-\\frac{||\\boldsymbol{x}-\\boldsymbol{z}||^2}{2\\sigma^2}\\right)$$\n",
    "\n",
    "The question now is whether there exists a feature map $\\boldsymbol{\\phi}$ such that the kernel $K$ defined above satisifies $K(\\boldsymbol{x},\\boldsymbol{z})=\\boldsymbol{\\phi}(\\boldsymbol{x})^T\\boldsymbol{\\phi}(\\boldsymbol{x})$. For this particular example the answer is yes. This kernel is the Gaussian kernel and corresponds to an infinite dimensional feature mapping $\\boldsymbol{\\phi}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aac0d42-e7ee-4312-872d-7a61cdf7fa04",
   "metadata": {},
   "source": [
    "### Necessary Conditions for Valid Kernels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da7adf48-8cfb-4193-aa5d-88cf44bc8d37",
   "metadata": {},
   "source": [
    "Recall from earlier we could construct the kernal as a matrix whose elements are given by $K_{ij} = K(\\boldsymbol{x}^{(i)},\\boldsymbol{y}^{(i)})$. This matrix is called the kernel matrix. If $K$ is a valid kernel (corresponds to some feature mapping) then the corresponding kernel matrix $\\boldsymbol{K} \\in \\mathbb{R}^{n \\times n}$ is symmetric positive semi-definite."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32848ec5-c872-4d98-a2e2-124b7e656596",
   "metadata": {},
   "source": [
    "### Sufficient Conditions for Valid Kernels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed916811-e3f8-4300-a8fe-058d3c82878d",
   "metadata": {},
   "source": [
    "The condition above is not only a necessary condition, but also a sufficient condition for $\\boldsymbol{K}$ to be a valid kernel (called a Mercer kernel). \n",
    "\n",
    "Let $\\boldsymbol{K}: \\mathbb{R}^{n} \\times \\mathbb{R}^{n} \\to \\mathbb{R}$ be given. Then for $\\boldsymbol{K}$ to be a valid (Mercer) kernel, it is necessary and sufficient that for any $\\{x^{(1)},\\ldots,x^{(m)}\\},\\,n < \\infty$, the corresponding kernel matrix is symmetric positive semi-definite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd2a2ce-86b3-4f2e-b60f-6c208114cd27",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
