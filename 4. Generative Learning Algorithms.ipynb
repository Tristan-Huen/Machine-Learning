{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb818ed5-d7ed-42af-a95f-87df954be7b3",
   "metadata": {},
   "source": [
    "# Generative Learning Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee6c66d-44fd-44c4-8bc5-68c4ba824bc5",
   "metadata": {},
   "source": [
    "The algorithms discussed before modeled $p(y|x;\\theta)$ but now we analyze a different type of learning algorithm. Algorithms that try to learn $p(y|x)$ directly or algorithms that try to learn mappings directly from the space of inputs $\\mathcal{X}$ to the labels $\\{0,1\\}$ are called discriminative learning algorithms. The new type of learning algorithms we will discuss are called generative learning algorithms and try to model $p(x|y)$ (and $p(y)$) instead.\n",
    "\n",
    "After modeling $p(y)$, called the class priors, and $p(x|y)$  our algorithm can then use Bayes Theorem to derive the posterior distribution on y given x:\n",
    "\n",
    "$$p(y|x)=  \\frac{p(x|y)p(y)}{p(x)}$$\n",
    "\n",
    "Here $p(x) = p(x|y = 1)p(y = 1) + p(x|y = 0)p(y = 0)$. This denominator does not to be calculated if $p(y|x)$ is being calculated to make a prediction. \n",
    "\n",
    "$$\\underset{y}{\\text{arg max }}p(y|x) = \\underset{y}{\\text{arg max }} \\frac{p(x|y)p(y)}{p(x)} = \\underset{y}{\\text{arg max }}p(x|y)p(y)$$ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "498a20e1-ef7c-4765-a551-5b9897e0cc80",
   "metadata": {},
   "source": [
    "## Gaussian Discriminant Analysis (GDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0548c141-b65f-4ba1-befc-99eea79efcd1",
   "metadata": {},
   "source": [
    "This model assumes $p(x|y)$ is distributed as a normal multivariate distribution. Also we drop the $x_0=1$ connection so we work with $\\boldsymbol{x} \\in \\mathbb{R}^{n}$ instead of $\\mathbb{R}^{n+1}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b998f3-26f6-426c-8458-17bc762e0dba",
   "metadata": {},
   "source": [
    "### The Multivariate Normal Distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60f83e2e-1862-4210-b35d-e717cbb9bddb",
   "metadata": {},
   "source": [
    "The multivariate normal distribution in $n$-dimensions is parameterized by a mean vector $\\boldsymbol{\\mu} \\in \\mathbb{R}^{n}$ and a covariance matrix $\\boldsymbol{\\Sigma} \\in \\mathbb{R}^{n\\times n}$ where $\\boldsymbol{\\Sigma}$ is symmetric positive semi-definite ($\\boldsymbol{\\Sigma} \\geq 0$). Written $\\mathcal{N}(\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma})$ its density is\n",
    "\n",
    "$$p(\\boldsymbol{x};\\boldsymbol{\\mu},\\boldsymbol{\\Sigma}) = \\frac{1}{(2\\pi)^{n/2}|\\boldsymbol{\\Sigma}|^{1/2}}\\exp\\left(-\\frac{1}{2}(\\boldsymbol{x}-\\boldsymbol{\\mu})^T\\boldsymbol{\\Sigma}^{-1}(\\boldsymbol{x}-\\boldsymbol{\\mu})\\right)$$\n",
    "\n",
    "For some random variable $\\boldsymbol{X} \\sim \\mathcal{N}(\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma})$ the mean is\n",
    "\n",
    "$$E[\\boldsymbol{X}] = \\int_{\\boldsymbol{x}} \\boldsymbol{x}p(\\boldsymbol{x};\\boldsymbol{\\mu},\\boldsymbol{\\Sigma})\\,d\\boldsymbol{x} = \\boldsymbol{\\mu}$$\n",
    "\n",
    "The covariance of a random variable $\\boldsymbol{Z}$ is defined as\n",
    "\n",
    "$$\\text{Cov}(\\boldsymbol{Z})=E\\left[(\\boldsymbol{Z} − E[\\boldsymbol{Z}])(\\boldsymbol{Z} − E[\\boldsymbol{Z}])^T\\right]$$ \n",
    "\n",
    "and for our case $\\text{Cov}(\\boldsymbol{Z}) = \\boldsymbol{\\Sigma}$\n",
    "\n",
    "The multivariate normal distribution's shape is controled by $\\boldsymbol{\\Sigma}$. If $\\boldsymbol{\\Sigma}$ is the identity matrix then scaling it results in either compression or expansion. For a $2\\times 2$ covariance matrix, the off diagonal elements control compression/expansion at a certain angle."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e7d6e7a-c913-447b-b842-cdbf1b9c945b",
   "metadata": {},
   "source": [
    "### GDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f3f3d3-ffdd-4d77-9f87-c73f56d258bc",
   "metadata": {},
   "source": [
    "When we have a classification problem in which the input features $\\boldsymbol{x}$ are continuous-valued random variables then we can use the GDA model. The model is\n",
    "\n",
    "\\begin{align}\n",
    "y &\\sim \\text{Bernoulli}(\\phi) \\\\[3pt]\n",
    "\\boldsymbol{x}|y=0 &\\sim \\mathcal{N}(\\boldsymbol{\\mu}_0, \\boldsymbol{\\Sigma}) \\\\[3pt]\n",
    "\\boldsymbol{x}|y=1 &\\sim \\mathcal{N}(\\boldsymbol{\\mu}_1, \\boldsymbol{\\Sigma})\n",
    "\\end{align}\n",
    "\n",
    "Which means our distributions are \n",
    "\\begin{align}\n",
    "p(y) &= \\phi^{y}(1-\\phi)^{1-y} \\\\[3pt]\n",
    "p(\\boldsymbol{x}|y=0) &= \\frac{1}{(2\\pi)^{n/2}|\\boldsymbol{\\Sigma}|^{1/2}}\\exp\\left(-\\frac{1}{2}(\\boldsymbol{x}-\\boldsymbol{\\mu}_0)^T\\boldsymbol{\\Sigma}^{-1}(\\boldsymbol{x}-\\boldsymbol{\\mu}_0)\\right) \\\\[3pt]\n",
    "p(\\boldsymbol{x}|y=1) &= \\frac{1}{(2\\pi)^{n/2}|\\boldsymbol{\\Sigma}|^{1/2}}\\exp\\left(-\\frac{1}{2}(\\boldsymbol{x}-\\boldsymbol{\\mu}_1)^T\\boldsymbol{\\Sigma}^{-1}(\\boldsymbol{x}-\\boldsymbol{\\mu}_1)\\right)\n",
    "\\end{align}\n",
    "\n",
    "The parameters of our model are $\\phi, \\boldsymbol{\\mu}_0, \\boldsymbol{\\mu}_1, \\boldsymbol{\\Sigma}$. So, to fit these parameters we have as usual our training set {(\\boldsymbol{x}^{(i)},y^{(i)})} for $i=1,2,\\ldots,m$ and we will maximize the log of the joint likelihood\n",
    "\n",
    "\\begin{align}\n",
    "\\ell(phi, \\boldsymbol{\\mu}_0, \\boldsymbol{\\mu}_1, \\boldsymbol{\\Sigma}) &= \\ln \\prod_{i=1}^{m}p(\\boldsymbol{x}^{(i)},y^{(i)}; \\phi, \\boldsymbol{\\mu}_0, \\boldsymbol{\\mu}_1, \\boldsymbol{\\Sigma})\n",
    "& \\ln \\prod_{i=1}^{m} p(\\boldsymbol{x}^{(i)} | y^{(i)};\\boldsymbol{\\mu}_0, \\boldsymbol{\\mu}_1, \\boldsymbol{\\Sigma})p(y^{(i)}; \\phi)\n",
    "\\end{align}\n",
    "\n",
    "Compared to discriminative learning algorithms we were maximizing the log of the conditional likelihood\n",
    "\n",
    "$$\\ell(\\boldsymbol{\\theta}) = \\ln \\prod_{i=1}^{m} p(y^{(i)} | \\boldsymbol{x}^{(i)} ; \\boldsymbol{\\theta})$$\n",
    "\n",
    "If we maximize $\\ell$ w.r.t the parameters we find the MLE of the parameters to be\n",
    "\n",
    "\\begin{align}\n",
    "\\phi &= \\frac{1}{m} \\sum_{i=1}^{m} 1\\{y^{(i)}=1\\} \\\\[3pt]\n",
    "\\boldsymbol{\\mu}_0 &= \\frac{\\sum_{i=1}^{m} 1\\{y^{(i)}=0\\} \\boldsymbol{x}^{(i)}}{\\sum_{i=1}^{m} 1\\{y^{(i)}=0\\}} \\\\[3pt]\n",
    "\\boldsymbol{\\mu}_1 &= \\frac{\\sum_{i=1}^{m} 1\\{y^{(i)}=1\\} \\boldsymbol{x}^{(i)}}{\\sum_{i=1}^{m} 1\\{y^{(i)}=1\\}} \\\\[3pt]\n",
    "\\boldsymbol{\\Sigma} &= \\frac{1}{m} \\sum_{i=1}^{m} \\left(\\boldsymbol{x}^{(i)}-\\boldsymbol{\\mu}_{y^{(i)}}\\right)\\left(\\boldsymbol{x}^{(i)}-\\boldsymbol{\\mu}_{y^{(i)}}\\right)^{T}\n",
    "\\end{align}\n",
    "\n",
    "The  $1\\{y^{(i)}=1\\}$ is called a indicator function such that,\n",
    "\n",
    "$$1\\{x \\in A\\} = \\begin{cases}1 & \\text{if } x \\in A \\\\ 0 & \\text{if } x \\notin A\\end{cases}$$\n",
    "\n",
    "Alternatively the notation $1_A(x)$ may be used. Additionally, the Iverson bracket notation can be used:\n",
    " \n",
    "$$[P] = \\begin{cases}1 & \\text{if } P \\text{ is true} \\\\ 0 & \\text{otherwise}\\end{cases}$$\n",
    "\n",
    "Essentially, GDA looks at each class (almost but not quite due to using the same covariance matrix) separately and fits a Gaussian to each example. Afterwards for each point we determine what class label it belongs to using Bayes Theorem. This will give a decision boundary which will be slightly different than the one for logisitic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0103b63-752f-4370-968f-69e628e4bd50",
   "metadata": {},
   "source": [
    "### GDA and Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc2cb2f-2557-47db-8268-c7b537f0837c",
   "metadata": {},
   "source": [
    "There is a relationship between GDA and logistic regression. If $p(y=1,\\boldsymbol{x}; \\phi, \\boldsymbol{\\mu}_0, \\boldsymbol{\\mu}_1, \\boldsymbol{\\Sigma})$ is viewed as a function of $\\boldsymbol{x}$ then it can be expressed in the form:\n",
    "\n",
    "$$p(y=1,\\boldsymbol{x}; \\phi, \\boldsymbol{\\mu}_0, \\boldsymbol{\\mu}_1, \\boldsymbol{\\Sigma}) = \\frac{1}{1+\\exp(-\\boldsymbol{\\theta}^T)\\boldsymbol{x}}$$\n",
    "\n",
    "where $\\boldsymbol{\\theta}$ is some appropriate function of $\\phi, \\boldsymbol{\\mu}_0, \\boldsymbol{\\mu}_1, \\boldsymbol{\\Sigma}$. This is exactly the form that logistic regression used to model $p(y=1|\\boldsymbol{x})$\n",
    "\n",
    "So if $p(\\boldsymbol{x}|y)$ is a multivariate gaussian with the same covariance matrix then $p(y|\\boldsymbol{x})$ follows a logistic function. However, the converse of this statement is not true. So, GDA makes stronger modeling assumptions about the data than logistic regression. When these modeling assumptions are correct, then GDA will find better fits to the data, and is a better model. Specifically, GDA is asymptotically efficient. On the other hand, by making significantly weaker assumptions, logistic regression is also more robust and less sensitive to incorrect modeling assumptions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3300aa59-4b6f-47ac-bd44-be2ff77d4fd7",
   "metadata": {},
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b57014bf-cebb-4f28-b586-ca61a3476ca0",
   "metadata": {},
   "source": [
    "In GDA the input features $\\boldsymbol{x}$ were continuous-valued random variables but now we analyze a different algorithm where the $x_j$ are discrete.\n",
    "\n",
    "Consider the case of classifying emails as spam or non-spam. This type of example is a part of a broader set of problems called text classification. Let's say we are given a training set which has a set of emails labeled as spam or non-spam. To begin we will construct the features vector.\n",
    "\n",
    "An email will be represented by a feature vector whose length is equal to the number of words in the dictionary. So, if an email contains the $j$-th word of the dictionary, then $x_j = 1$ and otherwise $x_j = 0$. The set of words encoded into the feature vector is called the vocabulary.\n",
    "\n",
    "With this feature vector we can build a generative model and thus, we must model $p(x|y)$. However, let's say the vocabulary has 50000 words. This means $\\boldsymbol{x} \\in \\{0,1\\}^{50000}$ and modeling $\\boldsymbol{x}$ as a multinomial distribution over $2^{50000}$ possible outcomes creates a $2^{50000-1}$ size parameter vector. This is way too many parameters.\n",
    "\n",
    "To model $p(x|y)$ we will assume that the $x_j$'s are conditionally independent given $y$. In other words the features are all mutually independent from eachother or the value of some $x_j$ does not influence the beliefs about the values of other $x_j$. We can state this mathematically. We know that\n",
    "\n",
    "$$p(x_1,\\ldots,x_{50000}) = p(x_1|y)p(x_2|y,x_1)p(x_3|y,x_1,x_2)\\dots p(x_{50000}|y,x_1,\\ldots,x_{49999})$$\n",
    "\n",
    "With this (strong) assumption we have\n",
    "\n",
    "$$p(x_1,\\ldots,x_{50000}) = p(x_1|y)p(x_2|y)p(x_3|y)\\dots p(x_{50000}|y) = \\prod_{j=1}^{n}p(x_j|y)$$\n",
    "\n",
    "This assumption is called the Naive Bayes assumption and obviously is not mathematically true hence the \"naive\" in the name. Even though this assumption is very strong the algorithm resulting from it still works well.\n",
    "\n",
    "Our model has the following parameters\n",
    "\\begin{align}\n",
    "\\phi_{j|y=1} = p(x_j=1|y=1) \\\\[3pt]\n",
    "\\phi_{j|y=0} = p(x_j=1|y=0) \\\\[3pt]\n",
    "\\phi_y = p(y=1)\n",
    "\\end{align}\n",
    "\n",
    "The first is just the probability of the $j$-th word appearing in the email given it was spam while the second is the probability of the $j$-th word appearing in the email given it was not spam. The last parameter is just the cost prior or the prior probability that the next email will be spam. Given a training set $\\{(\\boldsymbol{x}^{(i)},y^{(i)});i=1,\\ldots,m\\}$ the joint likelihood is\n",
    "\n",
    "$$\\mathcal{L}(\\phi_y, \\phi_{j|y=0}, \\phi_{j|y=1}) = \\prod_{i=1}^{m} p(\\boldsymbol{x}^{(i)},y^{(i)};\\phi_y, \\phi_{j|y=0}, \\phi_{j|y=1})$$\n",
    "\n",
    "Maximizing this w.r.t the parameters gives the MLEs\n",
    "\n",
    "\\begin{align}\n",
    "\\phi_{j|y=1} &= \\frac{\\sum_{i=1}^{m}1\\{x_{j}^{(i)}=1 \\land y^{(i)}=1 \\}}{\\sum_{i=1}^{m} 1\\{y^{(i)}=1\\}} \\\\[3pt]\n",
    "\\phi_{j|y=0} &= \\frac{\\sum_{i=1}^{m}1\\{x_{j}^{(i)}=1 \\land y^{(i)}=0 \\}}{\\sum_{i=1}^{m} 1\\{y^{(i)}=0\\}} \\\\[3pt]\n",
    "\\phi_y &= \\frac{1}{m}\\sum_{i=1}^{m}1\\{y^{(i)}=1\\}\n",
    "\\end{align}\n",
    "\n",
    "where $\\land$ is the logical AND. The parameters have a very natural interpretation such as $\\phi_{j|y=1}$ which is just the fraction of the spam ($y=1$)emails in which word $j$ appears.\n",
    "\n",
    "With all of these parameters we then calculate\n",
    "\n",
    "$$p(y=1|\\boldsymbol{x}) = \\frac{p(\\boldsymbol{x}|y=1)p(y=1)}{p(\\boldsymbol{x})} = \\frac{p(y=1)\\prod_{j=1}^{m}p(x_j | y=1)}{p(y=1)\\prod_{j=1}^{m}p(x_j | y=1) + p(y=0)\\prod_{j=1}^{m}p(x_j | y=0)} $$\n",
    "\n",
    "The Naive Bayes algorithm here was developed for binary-valued $x_j$ however, the generalization to $x_j \\in \\{1,2,\\ldots,k\\}$ is straightforward. For this we could just model $p(x_j|y)$ as multinomial rather than as a Bernoulli. Even if one of the input attributes was continuous valued we can discretize it. For example different ranges of values could correspond to each $\\{1,2,\\ldots,k\\}$. This is useful in the case when the continuous-valued attributes are not well-modeled by a multivariate normal distribution (like in GDA) so Naive Bayes could result in a better classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c5a5ef-0097-4bc3-b63f-1b205205dcfd",
   "metadata": {},
   "source": [
    "### Laplace Smoothing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e081fc6d-da4b-4978-b30e-d9fe664323d3",
   "metadata": {},
   "source": [
    "One problem exists with the current algorithm which we can fix to make it work much better especially with text classification. Consider the scenario in which a certain word, which has never appeared in the training set, is taken in by the algorithm. This would cause the two parameters $\\phi_{j|y=1}$ and $\\phi_{j|y=0}$ to both be zero. As a result the $p(y=1|\\boldsymbol{x})$ would be calculated as a $0/0$.\n",
    "\n",
    "Other than the $0/0$ problem it is a bad idea to estimate the probability of some event to be zero just because it hasn't been seen it before in your training set. As another example given the problem of estimating the mean of a multinomial random variable $z$ taking values in $\\{1,\\ldots,k\\}$, we can paramterize our multinomial with $\\phi_j = p(z=j)$. Given a set of $m$ independent observations $\\{z^{(1)},\\ldots,z^{(m)}\\}$, the MLEs are given by\n",
    "\n",
    "$$\\phi_j = \\frac{1}{m}\\sum_{i=1}^{m}1\\{z^{(i)}=j\\}$$\n",
    "\n",
    "As before, using these MLEs can result in some $\\phi_j$'s being zero. We can use Laplace smoothing to avoid this by replacing the above with\n",
    "\n",
    "$$\\phi_j = \\frac{1}{m+k}\\left(1+\\sum_{i=1}^{m}1\\{z^{(i)}=j\\}\\right)$$\n",
    "\n",
    "One can confirm that sum of these new $\\phi_j$ still equals 1. Additionally, $\\phi_j\\neq 0 \\, \\, \\forall j$ which fixes our zero problem.\n",
    "\n",
    "Back to the Naive Bayes classifier we can add Laplace smoothin to obtain the following estimates of the parameters:\n",
    "\n",
    "\\begin{align}\n",
    "\\phi_{j|y=1} &= \\frac{1+\\sum_{i=1}^{m}1\\{x_{j}^{(i)}=1 \\land y^{(i)}=1 \\}}{2+\\sum_{i=1}^{m} 1\\{y^{(i)}=1\\}} \\\\[3pt]\n",
    "\\phi_{j|y=0} &= \\frac{1+\\sum_{i=1}^{m}1\\{x_{j}^{(i)}=1 \\land y^{(i)}=0 \\}}{2+\\sum_{i=1}^{m} 1\\{y^{(i)}=0\\}} \\\\[3pt]\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb8d1d9b-a4bf-44b5-9919-91aa74fc7bca",
   "metadata": {},
   "source": [
    "### Event Models for Text Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43d72cfc-df97-4521-902f-742764f034d0",
   "metadata": {},
   "source": [
    "The Naive Bayes as we have shown here uses the (multi-variate) Bernoulli event model. This model assumed that the way an email is generated is that first it is randomly determined (according to $p(y)$) whether a spammer or non-spammer will send you your next message. Then, the person sending the email runs through the dictionary, deciding whether to include each word $j$ in that email independently and according to the probabilities $p(x_j=1|y)=\\phi_{j|y}$. Thus the probability of a message was given by\n",
    "\n",
    "$$p(y)\\prod_{j=1}^{n}p(x_j|y)$$\n",
    "\n",
    "We will describe a different model, called the Multinomial event model. Let $x_j$ denote the identity of the $j$-th word in the email. Thus, $x_j$ is now an integer taking values in $\\{1,\\ldots,|V|\\}$ where $|V|$ is the size of our vocabulary. An email of $n$ words is now represented by a vector $\\boldsymbol{x}$ of length $n$ (the size of this can vary by email). So, if the email starts with \"A moment...\" then $x_1 = 1$ (\"a\" is the first word in the dictionary), and $x_2 = 2500$ (if \"moment\" is the 2500th word in the dictionary).\n",
    "\n",
    "In the multinomial event model, we assume that the way an email is generated is via a random process where spam/non-spam is first determined as before (using $p(y)$). Then, the sender of the email writes the email by first generating $x_1$ from some multinomial distribution over words ($p(x_1|y)$). Next, the second word $x_2$ is chosen independently of $x_1$ but from the same multinomial distribution, and similarly for the other $x_j$ until all $n$ words of the email have been generated. So, the overall probability ofa message is given by\n",
    "\n",
    "$$p(y)\\prod_{j=1}^{n}p(x_j|y)$$\n",
    "\n",
    "This formula has the same form as previous one except that the terms in the formula have different meanings. In this case $x_j|y$ is now a multinomial distribution.\n",
    "\n",
    "The parameters for this new model are $\\phi_y = p(y)$, $\\phi_{k|y=1}=p(x_j=k|y=1)$ (for any $j$) and $\\phi_{k|y=0}=p(x_j=k|y=0)$. Note that we have assumed that $p(x_j|y)$ is the same for all values of $j$ or in other words the distribution according to which a word is generated does not depend on its position $j$ in the email.\n",
    "\n",
    "Given a training set $\\{(\\boldsymbol{x}^{(i)},y^{(i)});i=1,\\ldots,m\\}$ where $\\boldsymbol{x}^{(i)} = \\left(x_{1}^{(i)},x_{2}^{(i)},\\ldots,x_{n_i}^{(i)}\\right)$ (here $n_i$ is the number of words in the $i$-th training example) the likelihood of the data is given by\n",
    "\n",
    "\\begin{align}\n",
    "\\mathcal{L}(\\phi_y,\\phi_{k|y=1},\\phi_{k|y=0}) = \\prod_{i=1}^{m}p(\\boldsymbol{x}^{(i)},y^{(i)}) = \\prod_{i=1}^{m} \\left(\\prod_{j=1}^{n_i}p(x_{j}^{(i)}|y;\\phi_{k|y=1},\\phi_{k|y=0})\\right)p(y^{(i)};\\phi_y)\n",
    "\\end{align}\n",
    "\n",
    "Maximizing this yields the MLEs of the parameters:\n",
    "\n",
    "\\begin{align}\n",
    "\\phi_{k|y=1} &= \\frac{\\sum_{i=1}^{m}\\sum_{j=1}^{n_i}1\\{x_{j}^{(i)}=k \\land y^{(i)}=1 \\}}{\\sum_{i=1}^{m}1\\{y^{(i)}=1\\}n_i} \\\\[3pt]\n",
    "\\phi_{k|y=0} &= \\frac{\\sum_{i=1}^{m}\\sum_{j=1}^{n_i}1\\{x_{j}^{(i)}=k \\land y^{(i)}=0 \\}}{\\sum_{i=1}^{m}1\\{y^{(i)}=0\\}n_i} \\\\[3pt]\n",
    "\\phi_y &= \\frac{1}{m}\\sum_{i=1}^{m}1\\{y^{(i)}=1\\}\n",
    "\\end{align}\n",
    "\n",
    "And if Laplace smoothing is added\n",
    "\\begin{align}\n",
    "\\phi_{k|y=1} &= \\frac{1+\\sum_{i=1}^{m}\\sum_{j=1}^{n_i}1\\{x_{j}^{(i)}=k \\land y^{(i)}=1 \\}}{|V|+\\sum_{i=1}^{m}1\\{y^{(i)}=1\\}n_i} \\\\[3pt]\n",
    "\\phi_{k|y=0} &= \\frac{1+\\sum_{i=1}^{m}\\sum_{j=1}^{n_i}1\\{x_{j}^{(i)}=k \\land y^{(i)}=0 \\}}{|V|+\\sum_{i=1}^{m}1\\{y^{(i)}=0\\}n_i} \\\\[3pt]\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e84131-c1c2-47a6-9508-827fa7a351c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
