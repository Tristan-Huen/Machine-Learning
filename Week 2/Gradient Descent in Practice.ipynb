{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41f6ff4f",
   "metadata": {},
   "source": [
    "# Gradient Descent in Practice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c459fb2f",
   "metadata": {},
   "source": [
    "## Feature Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdde6148",
   "metadata": {},
   "source": [
    "The convergence rate of gradient descent also depends on the value of the ranges of the features. For the contours of the cost function these ranges if they are large or small can lead to skinny/wide or tall/short contours which can lead to a slower convergence rate. We can rescale the range of the features to have a comparable ranges of values which improves the rate of convergence. There are some ways to do rescaling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bfcf06f",
   "metadata": {},
   "source": [
    "### Divide by Maximum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c514d2",
   "metadata": {},
   "source": [
    "Given a feature range of $a\\leq x_1 \\leq b$ we can simply rescale this to $\\frac{a}{b} \\leq \\frac{x_1}{b} \\leq 1$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c9dfab",
   "metadata": {},
   "source": [
    "### Mean Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "572324d9",
   "metadata": {},
   "source": [
    "This centers the range around 0. Given a feature range of $a\\leq x_1 \\leq b$ let $\\bar{x}$ be the mean of this range. Then we have \n",
    "\n",
    "$$\\frac{a-\\bar{x}}{b-a} \\leq \\frac{x_1-\\bar{x}}{b-a} \\leq \\frac{b-\\bar{x}}{b-a} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ceeb3cb",
   "metadata": {},
   "source": [
    "### Z-score Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be809b64",
   "metadata": {},
   "source": [
    "Given a feature range of $a\\leq x_1 \\leq b$ let $\\bar{x}$ and $\\sigma$ be the mean and standard deviation respectively of this range. Then we have,\n",
    "\n",
    "$$\\frac{a-\\bar{x}}{\\sigma} \\leq \\frac{x_1-\\bar{x}}{\\sigma} \\leq \\frac{b-\\bar{x}}{\\sigma} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4894d126",
   "metadata": {},
   "source": [
    "A general rule of thumb for feature rescaling is get it between -1 and 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c1b08f6",
   "metadata": {},
   "source": [
    "## Convergence of Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db888bf2",
   "metadata": {},
   "source": [
    "To analyze the convergence of the gradient descent one may plot a graph of the cost function vs. iteration step. The resulting curve created is called the learning curve. The cost function should decrease after every iteration or in other words be monotonically decreasing. If it is not then the value of $\\alpha$ may be poorly chosen or a bug is present in the code. One can see this by looking at the cost function for a certain parameter and seeing if it zigzags around the minimum.\n",
    "\n",
    "Another way to analyze convergence is to use an automatic convergence test. Let $\\varepsilon>0$ be small. If the cost function decreases by $\\leq \\varepsilon$ in one iteration we can assume it converged."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd75a6ba",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b27583",
   "metadata": {},
   "source": [
    "New features can be created by combining or transforming original features. For example $x_3 = x_1 x_2$. This in turn allows for fitting of non-linear functions "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c6a2e1",
   "metadata": {},
   "source": [
    "## Polynomial Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b26a63",
   "metadata": {},
   "source": [
    "This involes regression of the form\n",
    "\n",
    "$$f_{\\vec{w},b} = \\sum_{i=1}^{n} w_i x_{1}^{i} + b$$\n",
    "\n",
    "Regression can still be done by writing each column of the matrix $X$ as the corresponding power e.g. $x_{1}^{2}$ in the second column of $X$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a5f9d81",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
