{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a8a3b74-b167-41c4-b4ea-f90f922c0ea5",
   "metadata": {},
   "source": [
    "# Overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ae58cad-725c-4100-88af-3284db09f333",
   "metadata": {},
   "source": [
    "- A model that underfits the data is said to have a high bias. \n",
    "- A model that overfits the data is said to have a high variance.\n",
    "\n",
    "There are some ways to fix overfitting:\n",
    "- Collecting more training data\n",
    "- Only use certain features\n",
    "- Regularization: reduce value of parameters instead of removing them"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd22423e-8087-4f9c-b4de-c184399e803c",
   "metadata": {},
   "source": [
    "## Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d5bf147-757e-405d-b9d5-852fd1a66c3a",
   "metadata": {},
   "source": [
    "Regularization typically involves adding an additional term, called a regularizer $R(\\theta)$ cost function such that\n",
    "\n",
    "$$J_{\\lambda}(\\theta)=J(\\theta)+\\lambda R(\\theta), \\quad \\lambda \\geq 0$$\n",
    "\n",
    "The $J_{\\lambda}(\\theta)$ is often called the regularized loss and $\\lambda$ is called the regularization parameter. The regularizer is usually chosen to be a measure of the complexity of the model. Regularized loss attempts to balance finding a model that is a good fit (small $J(\\theta)$) and has a small model complexity (small $R(\\theta)$). The balance is controlled by $\\lambda$. \n",
    "\n",
    "A commonly used regularizer is $R(\\theta)= \\frac{1}{2}||\\theta||_{2}^{2}$ (the $m$ can be added in) which is $\\ell^{2}$ regularization. The notation $||x||_{p}$, also known as the $\\ell^{p}$, means\n",
    "\n",
    "$$||x||_{p} = \\left(\\sum_{i=1}^{n} |x_i|^{p}\\right)^{\\frac{1}{p}} \\rightarrow ||x||_{p}^{p} = \\sum_{i=1}^{n} |x_i|^{p} $$\n",
    "\n",
    "The $\\ell^{2}$ regularization in deep learning this is often referred to as weight-decay due to following:\n",
    "\n",
    "$$\\vec{\\theta} := \\vec{\\theta} - \\alpha \\nabla J_{\\lambda}(\\theta) = \\vec{\\theta} - \\alpha \\lambda \\vec{\\theta} - \\alpha \\nabla J(\\theta) = (1-\\alpha \\lambda) \\vec{\\theta} - \\alpha \\nabla J(\\theta)$$\n",
    "\n",
    "The $(1-\\alpha \\lambda)$ acts as decaying term to the weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db292f8-152a-4196-b4e9-625709d54c41",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
