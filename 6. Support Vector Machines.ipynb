{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed0504ef-42b8-492f-82b7-9971c7502239",
   "metadata": {},
   "source": [
    "# Support Vector Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b735a3f-9d46-48a3-bcbe-46de50df21c9",
   "metadata": {},
   "source": [
    "## Margins"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd33f639-5e3d-40bd-b08e-0332f8b6e91e",
   "metadata": {},
   "source": [
    "For logisitic regression if $\\boldsymbol{\\theta}^{T}\\boldsymbol{x} \\gg 0$ then we are very confident that $y=1$. Similarly, if $\\boldsymbol{\\theta}^{T}\\boldsymbol{x} \\ll 0$ then we are very confident that $y=0$. So, it is not unnatural to seek some $\\boldsymbol{\\theta}$ so that for each training sample, these two points are reflected. This idea will be formalized later using the idea of functional margins\n",
    "\n",
    "Another interpretation is to consider a training set with a decision boundary ($\\boldsymbol{\\theta}^{T}\\boldsymbol{x}=0$ also called the separating hyperplane) separating the positive and negative cases. Points farther away from the decision boundary are points we are very confident with our prediction while those closer represent less confidence. Thus, it would be better if all points have a sufficient distance from the decision boundary. This idea will be formalized later using the idea of geometric margins\n",
    "\n",
    "We will use different notation to talk about support vector machines (SVMs). Consider a linear classification for binary classification with labels $y$ and features $\\boldsymbol{x}$. Our class labels will be $y \\in \\{-1,1\\}$ and are parameters will be paramterized by $\\boldsymbol{w},b$ (we also drop the convention of $x_0=1$). So, our classifier is\n",
    "\n",
    "$$h_{w,b}(\\boldsymbol{x}) = g(\\boldsymbol{w}^{T}\\boldsymbol{x}+b)$$\n",
    "\n",
    "Here $g(z)=1$ if $z \\geq 0$ and $g(z)=-1$ otherwise. So, we will directly predict $y=1$ or $y=-1$ without estimating $p(y=1)$ like logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a74f2e-7f4f-42c3-853b-e397e0852184",
   "metadata": {},
   "source": [
    "## Functional and Geometric Margins"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55dc457b-dc36-4171-9e3a-ae9f872b9c07",
   "metadata": {},
   "source": [
    "Given a training example $(\\boldsymbol{x}^{(i)},y^{(i)})$, the functional margin of $(\\boldsymbol{w},b)$ w.r.t the training example is\n",
    "\n",
    "$$\\hat{\\gamma}^{(i)} =y^{(i)\\boldsymbol{w}^{T}\\boldsymbol{x}^{(i)}+b}$$\n",
    "\n",
    "If $y^{(i)\\boldsymbol{w}^{T}\\boldsymbol{x}^{(i)}+b}>0$ then our prediction for the example is correct. A large functional margin means a confident and correct prediction.\n",
    "\n",
    "For a linear classifier with our choice of $g$ above there is one property of the functional margin that makes it not a good measure of confidence. Scaling the parameters $(\\boldsymbol{w},b)$ does not change $h_{w,b}$ so, $g$ and $h_{w,b}$ depend only on the sign and not the magnitude of $\\boldsymbol{w}^{T}\\boldsymbol{x}+b$. These same scaling factors will affect the functional margin and scale it accordingly. Thus, the freedom of scaling the parameters allows the functional margin to arbitrarily large without changing anything meaningful. One may try to impose a normalization condition such as $||\\boldsymbol{w}||_2$ (the Euclidean norm). Then we can replace $(\\boldsymbol{w},b)$ with $(\\boldsymbol{w}/||\\boldsymbol{w}||_2,b/||\\boldsymbol{w}||_2)$ and consider the functional margin of this instead. This idea will be revisited later.\n",
    "\n",
    "Given a training set $S = \\{(\\boldsymbol{x}^{(i)},y^{(i)}) | i=1,\\ldots,n\\}$ we define the function margin of $(\\boldsymbol{w},b)$ w.r.t $S$ as the smallest of the functional\n",
    "margins of the individual training examples. We denote this as $\\hat{\\gamma}$ and write\n",
    "\n",
    "$$\\hat{\\gamma} = \\underset{i=1,\\ldots,n}{\\text{min}} \\hat{\\gamma}^{(i)}$$\n",
    "\n",
    "Next we will talk about geometric margins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e487a3d-1a08-4566-a642-a35cdd406f22",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
