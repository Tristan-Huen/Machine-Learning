{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7421bb26-72ec-4498-901f-9a0fb4433845",
   "metadata": {},
   "source": [
    "# Generalized Linear Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1580280b-243b-4868-8db8-09fd3bcac7eb",
   "metadata": {},
   "source": [
    "For regression we had $y|x;\\theta \\sim \\mathcal{N}(\\mu, \\sigma^2)$ and for classification we $y|x;\\theta \\sim \\text{Bernoulli}(\\phi)$ for some $\\mu$ and $\\phi$ as functions of $x$ and $\\theta$. Both of these methods are special cases of Generalized Linear Models (GLM)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f331db39-70f0-4d1f-85b3-cebeb43c7fcb",
   "metadata": {},
   "source": [
    "# The Exponential Family"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "981acdc8-d5b4-4f12-94cf-769e7ea31a60",
   "metadata": {},
   "source": [
    "A class of distributions is in the exponential family if it can be written in the form\n",
    "\n",
    "$$p(y;\\eta) = b(y)\\exp(\\eta^{T}T(y)-a(\\eta))$$\n",
    "\n",
    "where $\\eta$ is called the natural/canonical parameter of the distribution, $T(y)$ is the sufficient statistic (we will often consider distributions where $T(y)=y$),$b(y)$ is base measure, and $a(\\eta)$ is the log partition function. The quantity $e^{-a(\\eta)}$ plays the role of a normalizing constant that makes sure the $p(y:\\eta)$ sums/integrates to over $y$ to 1.\n",
    "\n",
    "A fixed choice of $T, a$, and $b$ defines a family (or set) of distributions that is parameterized by $\\eta$. \n",
    "\n",
    "We can show that the Bernoulli distributions belong to this family. Let $\\text{Bernoulli}(\\phi)$ be the Bernoulli distribution with mean $\\phi$. Recall that this means $y \\in \\{0,1\\}$ such that $p(y;\\eta) = \\phi^{y}(1-\\phi)^{1-y}$. We will rewrite this as\n",
    "\n",
    "$$p(y;\\eta) = \\exp(y\\ln\\phi + (1-y)\\ln(1-\\phi)) = \\exp\\left(y\\ln\\left(\\frac{\\phi}{1-\\phi}\\right) + \\ln(1-\\phi)\\right)$$\n",
    " \n",
    "So we can see that $\\eta=\\ln(\\phi/(1-\\phi))$ and if we use this to solve for $\\phi$ we get $\\phi=1/(1+e^{-\\eta})$ which is just the sigmoid function. This will show up again later when trying to derive logistic regression as a GLM. The rest of quantities $T,a,$ and $b$ can be seen in the distribution showing that the Bernoulli distribution is an exponential family distribution. The normal distribution is just as easily proven.\n",
    "\n",
    "There are many other distributions that are members of the exponential family and the next section shows how to construct a model in which $y$ (given $x$ and $\\theta$) comes from any of these distributions. There several notable properties\n",
    "\n",
    "1. The maximum likelihood estimate (MLE) w.r.t $\\eta$ is concave. On the other hand the negative log likelihood (NLL) is convex\n",
    "2. $E[y;\\eta]=\\frac{\\partial a(\\eta)}{\\partial \\eta}$ is the mean of the distribution as parameterized by $\\eta$\n",
    "3. $\\text{Var}[y;\\eta]=\\frac{\\partial^2 a(\\eta)}{\\partial \\eta^2}$\n",
    "\n",
    "Another fact about GLMS is that they all follow the same learning update rule, namely (we use batch gradient descent here):\n",
    "\n",
    "$$\\vec{\\theta} := \\vec{\\theta} - \\frac{\\alpha}{m}\\sum_{i=1}^{m} \\left(h_{\\theta}(\\vec{x}^{\\,(i)})-y^{(i)}\\right)\\vec{x}^{\\,(i)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c652609b-ec05-4cfd-8511-eb014e010306",
   "metadata": {},
   "source": [
    "## Constructing GLMs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f96d9526-bdf3-4497-a8f6-3d3b073f950d",
   "metadata": {},
   "source": [
    "Consider a classification or regression problem where we would like to predict value of some random variable $y$ as a function of $x$. To derive a GLM for this we will make the following assumptions:\n",
    "\n",
    "1. $y | x; \\theta \\sim \\text{ExponentialFamily}(\\eta)$\n",
    "2. Given $x$ the goal is to predict the expected value of $T(y)$ given $x$. For most of the examples since $T(y)=y$ so we would like the prediction $h(x)$ to satisfy $h(x)=E[y|x]$.\n",
    "3. Natural parameter $\\eta$ and the inputs $x$ are related linearly $\\eta=\\theta^{T}x$ or $\\eta_i =\\theta_{i}^{T}x$ if $\\eta$ is vector valued (this is more of a design choice) \n",
    "\n",
    "The task you are attempting to do will give you the distribution. For regression over real numbers you use a normal, for binary classification you use a Bernoulli, and so on.\n",
    "\n",
    "For the case of regression one can imagine the line (technically hyperplane) given by $\\theta^T x$ and we assume that each point has a corresponding normal distribution. The data then is some sampled value from the points distribution and by using a GLM we can attempt to find the $\\theta$ which gave that line."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96ecd720-11c1-4c8b-9f21-ffa106f544fd",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b5b00b-41f1-4e33-822b-b934fae21f1f",
   "metadata": {},
   "source": [
    "Given that we are interested in binary classification, $y \\in \\{0,1\\}$, it seems natural to choose the Bernoulli family of distributions to model $y|x$. We can recall from earlier that $\\phi=1/(1+e^{-\\eta})$ and since  $y|x;\\theta \\sim \\text{Bernoulli}(\\phi)$ then $E[y|x;\\theta]=\\phi$ so\n",
    "\n",
    "$$h_{\\theta}(x)= E[y|x;\\theta] = \\phi = \\frac{1}{1+e^{-\\eta}} = \\frac{1}{1+e^{-\\theta^{T}x}}$$\n",
    "\n",
    "The first equality follows from assumption 2, the third from assumption 1, and the last from assumption 3.\n",
    "\n",
    "The function $g$ giving the distributionâ€™s mean as a function of the natural parameter, $g(\\eta)=E[T(y);\\eta]$ (for us $T(y)=y$ so $g(\\eta) = \\frac{\\partial a(\\eta)}{\\partial \\eta}$) is called the canonical response function. The inverse $g^{-1}$ is called the canonical link function. (Note that $g^{-1}$ and $g$ can have the opposite definitions elsewhere)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa1ed71-703c-488e-824a-3e6658e21c20",
   "metadata": {},
   "source": [
    "## Softmax Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d58e66bc-13f1-4c69-bf3b-eddb44320660",
   "metadata": {},
   "source": [
    "Softmax regression is another member of the GLM family and can be analyzed using the method above (very messy). However, another way to approach it is using Cross Entropy. Here we are looking at the problem of multiclass classification. Let $\\vec{x}\\,^{(i)}\\in \\mathbb{R}^{n}$ ,$k$ be the number of classes, $\\vec{y}=[\\{0,1\\}^{k}]$ be a vector such that an element is a 1 if it belongs to that class and 0 everywhere else (essentially a one hot vector). Next we will assume that each class has it's own set of parameters $\\vec{\\theta}_{c} \\in \\mathbb{R}^{n}$ where $c \\in \\{1,2,\\ldots,k\\}$. This can also be thought of as a matrix where each row represents the parameters of a certain class. Corresponding to each class is a decision boundary $\\theta_{c}^{T}x$. Given some $x$ we can plot a graph of $\\theta_{c}^{T}x$ vs $c$. What we will get is a certain value for each class and the value $\\theta_{c}^{T}x \\in \\mathbb{R}$ belongs to the logit space. Our goal now is to get a probability distribution over the classes and we can do so using the following steps.\n",
    "\n",
    "1. Exponentiate the logits $\\exp(\\theta_{c}^{T}x)$ to make all values positive\n",
    "2. Normalize by dividing everything by the sum of them $\\sum_{i=1}^{k} \\exp(\\theta_{i}^{T}x)$\n",
    "\n",
    "This creates a probability distribution $\\hat{p}(y)$\n",
    "\n",
    "$$\\hat{p}(y) = \\frac{e^{\\theta_{c}^{T}x}}{\\sum_{i=1}^{k} \\exp(\\theta_{i}^{T}x)}$$\n",
    "\n",
    "This is like our hypothesis function but it outputs a probability distribution over all classes instead of scalar or probability. Now the true output $y$ would give a probability distribution $p(y)$, called the label, which is 1 for a certain class and 0 elsewhere. The learning approach we now take is to minimize the \"distance\" between these two probability distributions or in other words get $\\hat{p}(y)$ to appear more like $p(y)$. What this is essentially doing is minimizing the cross entropy between them.\n",
    "\n",
    "$$H(p,\\hat{p})= -\\sum_{y \\in \\{1,2,\\ldots,k\\}}p(y)\\ln\\hat{p}(y)$$ \n",
    "\n",
    "We can then treat this as the loss (referred to as cross entropy loss) and do gradient descent on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc9df19e-5f9e-4e55-9ba3-95265666e432",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
